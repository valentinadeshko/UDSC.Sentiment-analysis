{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<h1 style=\"font-size:40px; font-family:Verdana\" align=\"center\"> UDS-Club Workshop </h1> -->\n",
    "<h2 style=\"font-size:30px; font-family:Verdana\" align=\"center\"> Nonlinear Algorithms: Tree-based and Gradient Boosting Models </h2>\n",
    "<img src='http://i.piccy.info/i9/666d78be04fbcf04fdb321d5953d1fa5/1492256847/123248/1137898/ua_parrots.jpg'/>\n",
    "<h4 style=\"font-size:16px; font-family:Verdana\" align=\"right\"> by Valentina Deshko <br> <pre>    2017-04-23</pre> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[1. Tree-based Models](##Tree-based-Models)  \n",
    "[Intro to the tree-based algorithms](#Intro-to-the-tree-based-algorithms)  \n",
    "[Constructing the decision tree](#Constructing-the-decision-tree)  \n",
    "[Random Forest (brief overview)](#Random-Forest-(brief-overview))  \n",
    "[Extremely Randomize Trees](#Extremely-Randomize-Trees)  \n",
    "[2. Gradient Boosting Algorithms](#Gradient-Boosting-Algorithms)  \n",
    "[How does boosting work](#How-does-boosting-work)  \n",
    "[Understanding of LightGBm parameters](#Understanding-of-LightGBm-parameters)   \n",
    "[Understanding of XGBoost parameters](#Understanding-of-XGBoost-parameters)   \n",
    "[General approach for parameters tuning](#General-approach-for-parameters-tuning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Tree-based Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to the tree-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is one of the most popular supervised machine learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Tree models where the target variable can take a finite set of values are called classification trees. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n",
    "\n",
    "There are many specific decision-tree algorithms. Notable ones include:\n",
    " - ID3 (Iterative Dichotomiser 3)\n",
    " - C4.5 (successor of ID3)\n",
    " - CART (Classification And Regression Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn uses an optimised version of the CART (Classification and Regression Trees) algorithm, that is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node. <br> Let us consider how we can build a decision tree using our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import  metrics, ensemble\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train data\n",
    "data = pd.read_csv(\"../data/movie_reviews.csv\", sep=\",\")\n",
    "# test data\n",
    "test_data = pd.read_csv(\"../data/test.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize data\n",
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(word) for word in word_list]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data on train and test \n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        data.text, \n",
    "        data.label,\n",
    "        test_size=0.1, \n",
    "        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "#buid a classifier\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=17)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3),\n",
    "                              analyzer = 'word', binary = True, max_df = 0.75, vocabulary=None)\n",
    "\n",
    "#make a pipeline and fit the model\n",
    "pipeline = Pipeline([('vectorizer', vectorizer), \n",
    "                     ('classifier', clf_tree)])\n",
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.642225280126\n",
      "F1 score_neg:  0.31234256927\n"
     ]
    }
   ],
   "source": [
    "#validation accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "print (\"Validation Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score_neg: \", f1_score(y_test, y_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tree visualization (don't forget to install pydot library before)\n",
    "\n",
    "export_graphviz(pipeline.named_steps['classifier'], \n",
    "out_file='sentiment_tree.dot', filled=True)\n",
    "\n",
    "#transform our picture from .dot to .png format\n",
    "!dot -Tpng sentiment_tree.dot -o ../small_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://piccy.info/view3/11093446/0264747ab85fba81e151d28967820cd0/\" target=\"_blank\"><img src=\"http://i.piccy.info/i9/18234d3aac91cc89c6680134b321d6d7/1492623923/49692/1138985/small_tree1.png\" alt=\"Piccy.info - Free Image Hosting\" border=\"0\" /></a><a href=\"http://i.piccy.info/a3c/2017-04-19-17-45/i9-11093446/786x359-r\" target=\"_blank\"><img src=\"http://i.piccy.info/a3/2017-04-19-17-45/i9-11093446/786x359-r/i.gif\" alt=\"\" border=\"0\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which term were choosen to split our data in nodes. <br>\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Constructing the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, we can say that decision tree is a simple flowchart that selects labels for target values.\n",
    "This flowchart consists of decision nodes, which check feature values, and leaf nodes, which assign labels. To choose the label for target value, we begin at the flowchart's initial decision node, known as its root node. \n",
    "\n",
    "Decision trees are formed by a collection of rules based on features in the modeling data set:\n",
    "\n",
    "* Rules based on features' values are selected to get the best split to differentiate observations based on the dependent/target variable<br/>\n",
    "* Once a rule is selected and splits a node into two, the same process is applied to each \"child\" node (i.e. it is a recursive procedure)<br/>\n",
    "* Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned)<br/>\n",
    "* Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature, that is selected to get the best split intuitively corresponds to the idea of information gain based on entropy. \n",
    "The Shannon entropy (named after Claude Shannon, the \"father of information theory\") for a system with N possible states is defined as follows:\n",
    "$$\\Large S = -\\sum_{i=1}^{N}p_ilog_2p_i$$\n",
    "\n",
    "where p - probability of each label;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  calculate entropy for our root node and make sure that we get the same result as on our plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776924394570736\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "S = - 56629/137349*log(56629/137349, 2) - 80720/137349*log(80720/137349, 2)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By itself this value doesn't say a lot to us. So, lets split our data and calculate an Information Gain (which is the difference between the entropies before and after split)\n",
    "\n",
    "$$\\Large IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{|N_i|}{N}S_i,$$\n",
    "\n",
    "N_i - subset of N, which has value i;<br>\n",
    "q - number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 \n",
    "please add the required data and calculate Information Gain after the decision tree' first split on our plot using the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IG = S - N1/137349*S1 - N2/137349*S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<details>\n",
    "  <summary>Click to see answer</summary>\n",
    "      <pre>\n",
    "          <code>\n",
    "              IG = 0.03             \n",
    "          </code>\n",
    "      </pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main parameters:\n",
    "* max_depth – the maximum depth of the tree;\n",
    "* max_features - the number of features to consider when looking for the best split;\n",
    "* min_samples_leaf – the minimum number of samples required to split an internal node.<br>\n",
    "You can use cross-validation to tune model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Decision Trees Pros\n",
    "* Simple to understand and to interpret (uses a white box model). Trees can be visualised.\n",
    "* Requires little data preparation.\n",
    "* Able to handle both numerical and categorical data.\n",
    "* Fast training \n",
    "\n",
    "### Decision Trees Cons\n",
    "* Decision-tree learners can create over-complex trees that do not generalise the data well (overfitting).\n",
    "* Can be unstable because small variations in the data might result in a completely different tree being generated.\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree. <br>\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Algorithms: Random Forest and Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what should we do if we can not improve model accuracy using only one algorithm? We should try more advanced machine learning techniques called \"ensembles\". \n",
    "Simply put we can say that ensemble is predictions' aggregation of several basic algorithms aiming higher accuracy. <br>\n",
    "\n",
    "Lets start learning ensemble algorithms with notion of Bagging.<br>\n",
    "A Bagging is an ensemble meta-estimator that fits base classifiers/regressors each on random subsets of the original dataset and then aggregate their individual predictions (in classification case by voting, in regression case by averaging) to form a final prediction.\n",
    "\n",
    "<img src=\"http://i.piccy.info/i9/293f6705e969ef92944f31c6c08ae1c6/1492767761/123500/1138985/bagging.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (brief overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees (decision trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance).\n",
    "Random decision forests correct for decision trees' habit of overfitting to their training set. <br> \n",
    "\n",
    "So we can say that Random Forest is like bootstrapping algorithm with Decision tree (CART) model. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is called random subspace method or sometimes \"feature bagging\", and it attempts to reduce the correlation between estimators (decision trees) in an ensemble by training them on random samples of features instead of the entire feature set. <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.745298473232\n",
      "F1 score_neg:  0.612191958495\n"
     ]
    }
   ],
   "source": [
    "# RF code and example\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 50, min_samples_leaf=3, n_jobs = -1,\n",
    "                                       random_state = 17)\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3),\n",
    "                              analyzer = 'word', binary = True, max_df = 0.75, vocabulary=None)\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', vectorizer), \n",
    "                     ('classifier', rf_classifier)])\n",
    "model = pipeline.fit(X=X_train, y=y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print (\"Validation Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score_neg: \", f1_score(y_test, y_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.63339587242\n",
      "F1 score_neg:  0.444412851862\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.29      0.44      5330\n",
      "          1       0.58      0.97      0.73      5330\n",
      "\n",
      "avg / total       0.75      0.63      0.59     10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(test_data.text)\n",
    "print (\"Test Accuracy: \", accuracy_score(test_data.label, y_pred_test))\n",
    "print(\"F1 score_neg: \", f1_score(test_data.label, y_pred_test, pos_label = 0))\n",
    "\n",
    "print(classification_report(test_data.label, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main parameters:\n",
    "* n_estimators — the number of trees in the forest (by default=10);\n",
    "* max_features — the number of features to consider when looking for the best split;\n",
    "* criterion — the function to measure the quality of a split ('mse' for regression, gini' or 'entropy' for classification);\n",
    "* min_samples_leaf — the minimum number of samples required to be at a leaf node;\n",
    "* max_depth — the maximum depth of the tree.\n",
    "\n",
    "You could find more information about Randon Forest (and other ensembles) parameters [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Pros:\n",
    "* it is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier;\n",
    "* runs efficiently on large databases;\n",
    "* gives estimates of what variables are important in the classification;\n",
    "* has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing;\n",
    "* has methods for balancing error in class population unbalanced data sets;\n",
    "\n",
    "### Random Forest Cons:\n",
    "* produces a lower accurary on 'sparse' data (e.g. text, bag of words, sparse matrix);\n",
    "* can be overfitted for some datasets with noisy classification/regression tasks;\n",
    "* difficult to interpret.\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomize Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or Extra Trees are very similar to Random Forest algorithm, but use differenrt approach to constructing the decion trees:\n",
    "\n",
    "* each tree is built from the complete learning sample (doesn't apply the bagging procedure to construct a set of the training samples for each tree);\n",
    "* for each of the features (randomly selected at each interior node) a discretization threshold (cut-point) is selected at random to define a split, instead of choosing the best cut-point based on the local sample (as in Tree Bagging or in the Random Forests methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.77137802241\n",
      "F1 score_neg:  0.734333358715\n"
     ]
    }
   ],
   "source": [
    "# Extra Tree Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=50, \n",
    "                             max_leaf_nodes=None, \n",
    "                             min_samples_leaf=3, \n",
    "                             random_state=1,\n",
    "                             n_jobs=-1,\n",
    "                             class_weight='balanced'\n",
    "                            )\n",
    "\n",
    "# create pipeline\n",
    "model = Pipeline([('vectorizer', vectorizer),\n",
    "        ('clf', clf)])\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print (\"Validation Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score_neg: \", f1_score(y_test, y_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.776547842402\n",
      "F1 score_neg:  0.754128819158\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.69      0.75      5330\n",
      "          1       0.73      0.87      0.80      5330\n",
      "\n",
      "avg / total       0.79      0.78      0.77     10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(test_data.text)\n",
    "print (\"Test Accuracy: \", accuracy_score(test_data.label, y_pred_test))\n",
    "print(\"F1 score_neg: \", f1_score(test_data.label, y_pred_test, pos_label = 0))\n",
    "\n",
    "print(classification_report(test_data.label, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Gradient Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does boosting work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The common ensemble techniques like random forests rely on simple averaging of models in the ensemble. The family of boosting methods is based on a different, constructive strategy of ensemble formation. The main idea of boosting is to add new models to the ensemble sequentially. <br> \n",
    "\n",
    "In gradient boosting  the learning procedure consecutively fits new models to provide a more accurate estimate of the response variable. The principle idea behind this algorithm is to construct the new base-learners to be maximally correlated with the negative gradient of the loss function, associated with the whole ensemble. Simply put, the next algorithm tries to fix the error of the previous one. \n",
    "\n",
    "Briefly consider the constructing of gradient boosting algorithm:\n",
    "\n",
    "* Step 1. Applying a base learning algorithm.\n",
    "* Step 2. Initially all points have same weight (denoted by their size). After the first iteration the points classified correctly  are given a lower weight and vice versa.\n",
    "* Step 3. The next algorithm focuses on high weight points and try to classificate them correctly.\n",
    "* Step 4. Iterate steps 2 and 3  till the limit of base learning algorithm is reached or higher accuracy is achieved or no longer improves on an external validation dataset.\n",
    "\n",
    "\n",
    "We will consider XGBoost and LightGbm: gradient boosting frameworks that use tree based learning algorithms. \n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Understanding of LightGBm parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a model using XGBoost or LightGbm is easy. But improving the model is difficult. These algorithms use multiple parameters. To improve the model, parameter tuning is must. It is very difficult to get answers to practical questions like – Which set of parameters should be tuned ? Let's take a look on general GBm parameters\n",
    "\n",
    "All GBM parameters can be divided in three main groups:\n",
    "1. Tree-Specific Parameters;\n",
    "2. Boosting Parameters;\n",
    "3. Miscellaneous Parameters. \n",
    "\n",
    "\n",
    "The optimal parameters of a model can depend on many scenarios. So it is impossible to create a comprehensive guide for doing so. We will try to provide some guideline for parameters in gradient boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on LightGbm parameters tuning\n",
    "\n",
    "### For better accuracy\n",
    "\n",
    "* Use large `num_leaves` - number of leaves in one tree (may cause over-fitting) <br>\n",
    "* Use large `max_bin` (max number of bin that feature values will bucket in) <br>\n",
    "* Use small `learning_rate` (shrinkage rate) with large `num_iterations` (only used in prediction task, used to how many trained iterations will be used in prediction) <br>\n",
    "* Use bigger training data <br>\n",
    "\n",
    "\n",
    "### For faster \n",
    "\n",
    "* Use bagging by set `bagging_fraction` (will random select part of data)  and `bagging_freq` (Frequency for bagging) <br>\n",
    "* Use feature sub-sampling by set `feature_fraction` (will random select part of features on each iteration)\n",
    "* Use small `max_bin`\n",
    "* Use `save_binary` (will save the data set(include validation data) to a binary file)  to speed up data loading in future learning <br>\n",
    "* Use parallel learning, refer to [parallel learning guide](https://github.com/Microsoft/LightGBM/wiki/Parallel-Learning-Guide) \n",
    "\n",
    "### Deal with over-fitting\n",
    "\n",
    "* Use small `max_bin` <br>\n",
    "* Use small `num_leaves` <br>\n",
    "* Use `min_data_in_leaf` (minimal number of data in one leaf) <br>\n",
    "* Use bagging by set `bagging_fraction` and `bagging_freq` <br>\n",
    "* Use feature sub-sampling by set `feature_fraction` <br>\n",
    "* Use bigger training data <br>\n",
    "* Try `lambda_l1`, `lambda_l2` and `min_gain_to_split` to regularization <br>\n",
    "* Try `max_depth` to avoid growing deep tree (tree still grows by leaf-wise) <br>\n",
    "\n",
    "To learn more about tuning LightGbm hyperparameters please click [this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.792674136688\n"
     ]
    }
   ],
   "source": [
    "# LightGbm classifier\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    colsample_bytree=0.7, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=-1, \n",
    "    min_child_samples=15, \n",
    "    n_estimators=150, \n",
    "    reg_lambda=1, \n",
    "    scale_pos_weight=0.85, \n",
    "    subsample_for_bin=5000\n",
    "    )\n",
    "\n",
    "vectorizer = HashingVectorizer(tokenizer=tokenize, ngram_range=(1, 3),\n",
    "                              analyzer = 'word', binary = True)\n",
    "# create pipeline                                                                                     \n",
    "\n",
    "pipeline = Pipeline([('vectorizer', vectorizer),\n",
    "                  ('clf', clf)])\n",
    "# fit model\n",
    "model = pipeline.fit(X=X_train, y=y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print (\"Validation Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.73574108818\n",
      "F1 score_neg:  0.692702083561\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.60      0.69      5330\n",
      "          1       0.68      0.88      0.77      5330\n",
      "\n",
      "avg / total       0.76      0.74      0.73     10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(test_data.text)\n",
    "print (\"Test Accuracy: \", accuracy_score(test_data.label, y_pred_test))\n",
    "print(\"F1 score_neg: \", f1_score(test_data.label, y_pred_test, pos_label = 0))\n",
    "\n",
    "print(classification_report(test_data.label, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding of XGBoost parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notes on XGBoost parameters tuning\n",
    "\n",
    "Usually we start tuning parameters with these first: <br>\n",
    "`n_estimators` - number of algorithms; <br>\n",
    "`max_depth` - maximum depth of a tree, increase this value will make the model more complex; <br>\n",
    "`min_child_weight` - minimum sum of instance weight (hessian) needed in a child <br>\n",
    "`gamma` - minimum loss reduction required to make a further partition on a leaf node of the tree <br>\n",
    "as they will have the highest impact on model outcome\n",
    "\n",
    "\n",
    "\n",
    "### Control Overfitting\n",
    "When you observe high training accuracy, but low tests accuracy, it is likely that you encounter overfitting problem.\n",
    "\n",
    "There are in general two ways that you can control overfitting:\n",
    "\n",
    "* The first way is to directly control model complexity\n",
    "This include `max_depth` - maximum depth of a tree, increase this value will make the model more complex; <br>\n",
    "`min_child_weight` - minimum sum of instance weight needed in a child; <br>\n",
    "`gamma` - minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "\n",
    "* The second way is to add randomness to make training robust to noise\n",
    "This include `subsample` - subsample ratio of the training instance,\n",
    "`colsample_bytree` - subsample ratio of columns when constructing each tree. <br>\n",
    "You can also reduce stepsize `eta`  -step size shrinkage used in update to prevents overfitting,\n",
    "but needs to remember to increase `num_round` (the number of rounds for boosting)  when you do so.\n",
    "\n",
    "\n",
    "### Handle Imbalanced Dataset\n",
    "There are two ways to improve it:\n",
    "\n",
    "* If you care only about the ranking order (AUC) of your prediction\n",
    "Balance the positive and negative weights, via `scale_pos_weight`\n",
    "Use AUC for evaluation\n",
    "* If you care about predicting the right probability\n",
    "In such a case, you cannot re-balance the dataset\n",
    "In such a case, set parameter `max_delta_step` to a finite number (say 1) will help convergence <br>\n",
    "\n",
    "More information you can find in this [nice article](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.796867833038\n"
     ]
    }
   ],
   "source": [
    "# create a pipeline\n",
    "clf = xgb.XGBClassifier(learning_rate=0.2, \n",
    "                        n_estimators = 200,\n",
    "                        objective='binary:logistic', \n",
    "                        colsample_bytree=0.8,\n",
    "                        min_child_weight=7,\n",
    "                        max_depth=10,\n",
    "                        scale_pos_weight=0.8\n",
    "                        )\n",
    "vectorizer = HashingVectorizer(tokenizer=tokenize, ngram_range=(1, 3),\n",
    "                              analyzer = 'word', binary = True)\n",
    "pipeline = Pipeline([('vectorizer', vectorizer), \n",
    "                     ('classifier', clf)])\n",
    "\n",
    "# fit model\n",
    "model = pipeline.fit(X=X_train, y=y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print (\"Validation Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.754315196998\n",
      "F1 score_neg:  0.722651699672\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.64      0.72      5330\n",
      "          1       0.71      0.87      0.78      5330\n",
      "\n",
      "avg / total       0.77      0.75      0.75     10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(test_data.text)\n",
    "print (\"Test Accuracy: \", accuracy_score(test_data.label, y_pred_test))\n",
    "print(\"F1 score_neg: \", f1_score(test_data.label, y_pred_test, pos_label = 0))\n",
    "\n",
    "print(classification_report(test_data.label, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General approach for parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using scikit-learn we can perform a grid search of the  model parameters.<br> \n",
    "Will be using  5000 reviews from our whole data and basic XGBoost algorithm just to demonstrate a basic prinsiple how to tune XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select 5000 reviews from data\n",
    "train, test = train_test_split(\n",
    "        data, \n",
    "        train_size=5000, \n",
    "        random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifier\n",
    "clf = xgb.XGBClassifier(learning_rate=0.1, max_depth=5)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3),\n",
    "                              analyzer = 'word', binary = True, max_df = 0.75, vocabulary=None)\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', vectorizer), \n",
    "                     ('classifier', clf)])\n",
    "\n",
    "#set the parameters range that we try to tune\n",
    "parameters = {\n",
    "    'classifier__n_estimators': (1, 20, 50, 100, 150, 200, 250)}\n",
    "\n",
    "#use cross-validation to evaluate the performance\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "#use gridsearch over specified parameter values for an estimator\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring=\"accuracy\", cv=kfold)\n",
    "grid_result = grid_search.fit(X=train.text, y=train.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.719200 using {'classifier__n_estimators': 250}\n",
      "0.641600 (0.005010) with: {'classifier__n_estimators': 1}\n",
      "0.682600 (0.003730) with: {'classifier__n_estimators': 20}\n",
      "0.696800 (0.003562) with: {'classifier__n_estimators': 50}\n",
      "0.713400 (0.001904) with: {'classifier__n_estimators': 100}\n",
      "0.716200 (0.005331) with: {'classifier__n_estimators': 150}\n",
      "0.717800 (0.003517) with: {'classifier__n_estimators': 200}\n",
      "0.719200 (0.005964) with: {'classifier__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "accuracy = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(accuracy, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x29d441b54a8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAFOCAYAAADZxVr8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcnXV99//XJ7MkmexACAkJJGyyBzWAIlYKLoAEqq0K\n7lutd6vVLlq1tnrf3rYqdrFq9YeAtYJwW0UNiIA7BAUhmEBCQGL2TFaSkD2zfX5/nGvIyTCTTJYz\n58zM6/l4zCPnWs51PnNdHOft9/u9rm9kJpIkSapNQ6pdgCRJknpmWJMkSaphhjVJkqQaZliTJEmq\nYYY1SZKkGmZYkyRJqmGGNUmDQkR8LCKur3YdknSgDGtSlUTEyIhYGhFvKls3KiKWR8SflK2bERF3\nRMSmiNgcEY9HxKcjYlyx/e0R0R4R24qfxRHxvypc+0URsbKSn3EouqsvM/8pM99doc9bGhEvr8Sx\nKykipkVER0R8pdq1SOqZYU2qkszcBvwZ8O8RMb5Y/Tng4cz8DkBEXAD8ArgfODUzxwKXAm3A9LLD\n/TozR2bmSOCPgc9FxPP75jfRoYiI+ip+/FuBTcAbImJoX35wlX9vqV8xrElVlJl3Az8E/iMiLgJe\nD/x52S6fA76emf+cmWuL9yzPzE9k5i96OOZvgYXAaZ3rIuLKiFhQtMz9IiLKt51WrNtc7HNl2bbL\ni5a8rRGxKiL+NiJGAD8CJpW15k3a3+8aEf8VEV+OiB8Wx3swIk7sxftOjYgfR8TGiHgyIl5/MPVF\nxCcj4qbifVMjIiPiHRGxomi1fG9EnBsRjxbn4ktln3NiRPwsIp6OiA0RcXNEjC22fRM4Dri9+KwP\n9+KcL42Iv4uIR4HtEVFfLK8qfpcnI+KSbs7F+RGxJiLqyta9pjgOEXFeRDwcEVsiYm1E/Os+zmtQ\nCmsfB1qBmV22n1F23tdGxMeK9XVR6lL+fVHrnIiYUnZO68uO8YuIeHfx+u0RcX9E/FtEPA18cl/n\ntXjPlIi4LSLWF/t8KSIai5rOKtvv6IjYEXv+T480sGSmP/74U8UfYBywGtgAvKNs/QigHbhoP+9/\nOzC7bPlcYDNwSrF8CrAdeAXQAHwYWAQ0FsuLgI8VyxcDW4HnFe9dDby0rM4XFK8vAlZ2qeNCYPM+\n6vwv4GngPKAeuBm4dT+/2whgBfCO4j3PL87T6QdR3yeBm4rXU4EEvgoMA14J7AK+DxwNHAusA15W\n7H9Scf6GAuOBe4F/Lzv2UuDlZcs9nvOy/ecCU4DhwPOK33NSWX0n9nBOfg+8omz5f4CPFK9/Dbyl\neD0SeNE+zu1Lgd3FefsicHvZtlHFuf2b4vyMAs4vtn0IeKyoOSi18B5Zdk7ry47zC+DdZf+dtgHv\nL67l8H2dV6AOmAf8W/HfwTDgwmLbfwKfLfucD5TX748/A+3HljWpyjJzE7AAaAJuK9s0jlLr95rO\nFRHxuaKlZntEfLxs3xcV67cCvwG+CTxVbHsD8MPM/HFmtgKfp/SH8gLgRZT+qH8mM1sy82fAHcA1\nxXtbgdMjYnRmbsrMR/bxe8zOUjftvnwvM3+TmW2Uwto5+9n/CmBpZn49M9uy1Gr4XeB1B1pfDz6V\nmbsy8x5K4eqWzFyXmauA+yiFQzJzUXH+dmfmeuBfgZft47j7Oued/iMzV2TmTkqhfGjxuzRk5tLM\n/H0Px76F4vpExCjg8mJd5/k4KSKOysxtmfnAPmp8G/Cj4r+/bwGXRsTRxbYrgDWZ+S/F+dmamQ8W\n294NfDwzn8ySeZn59D4+p1xzZn6xuJY793NezwMmAR/KzO1FHbOLbd8ArilaBwHeQum/eWlAMqxJ\nVRYRb6bUKvET4LNlmzYBHcDEzhWZ+eEiEH2PUutEpwcyc2xmjgKOAc4A/qnYNglYVnaMDkqtOMcW\n21YU6zotK7ZBafzb5cCyiPhlRLz40H7bPcET2EEpKO7L8cD5RRDdHBGbgTdR+h0PR31ry17v7GZ5\nJEBETIiIW4tuyi3ATcBR+zjuvs55pxVl2xcBH6TU+reu+Kyeupa/Bbw2SmPMXgs8kpmdn/UuSq16\nT0TEQxFxRXcHiIjhlALvzcXn/xpYDryx2GUKpRa87uxr2/6sKF/Yz3mdAiwrgv1eiuC4A7goIk6l\n1EI36yBrkmqeYU2qoqIl49+AP6V0s8HrI+KlAJm5HXiQ0h/kXsvS2LbvsmcMUjOl0NP5mUHpD+Gq\nYtuUiCj/34Ljim1k5kOZeRWlrsHvA9/u/JgDqekQrAB+WQTRzp+Rmfm/+ri+fyqOeVZmjgbeTKkL\nsFPXz9vXOe/2PZn5rcy8sHhfsndwL9/vcUpB8DJK4epbZdueysxrKJ2PzwLfidIYvq5eA4wG/rMY\nA7eGUpB8W7F9BXBCd59fbOturOH24t+msnXHdNmn63na13ldARwXPd+I8I1i/7cA38nMXT3sJ/V7\nhjWpur4EfD8zf56ZqymNbfpa7Lkz78PAOyPiI51dVBExGZjW0wEj4khKf4wXFKu+Dbw6Ii6JiAZK\n45B2A7+iFAZ3AB+OiIYo3eQwE7i1GMj9pogYU3TlbaHU0gelFqgjI2LMYToPPbkDOCUi3lLU1xCl\nmwBO6+P6RgHbgGci4lhK47bKrWXvcLOvc/4cEfG8iLi4uO67KLXqdXS3b+FblMZp/QGlMWudx3lz\nRIwvWvI2F6u7O87bgBuBsyh1RZ8DvASYXgzcvwOYGBEfjIihUXqkzPnFe68HPhURJ0fJ2RFxZNGN\nuQp4c3ETwjvpPtSV29d5/Q2lcXOfiYgRETEsIl5Stv0mSv+dvxn47/18jtSvGdakKomIP6I0KP/Z\nP1CZeT2lVpl/LJZnUxr0/wfA74puwLsoDdz+YtnhXhzFnY+U7gRdT2kgN5n5JKU/aF+kNDh/JjCz\nGKPWUixfVmz7T+CtmflEcdy3AEuLLqr3UuqCpNh+C7C46J6cFBEvLT7/sMnMrZQG/19dnJc1lFqM\nOsNsr+s7xFL+N/AC4BlKd+/e1mX7PwMfLz7rb/d1zns4/lDgM8W+ayi1jH10H/XcQmls188yc0PZ\n+kuBBcV1+AJwdTEm7llFKLqE0kD+NWU/cyj9t/W24ry/oqh7DaXxj39YHOJfKYXReygF5BsojceD\nUgvxhyjdSHIGPYTTMj2e18xsLz7/JEpdtCspjQXs3L4CeIRSy9x9+/kcqV+LzL7qzZAk6fCJiBsp\n3bTw8f3uLPVjPpRQktTvRMRUSuM5ffizBryKdoNGxKVRerjjooj4SDfbPxQRc4uf+VGaMueIsu11\nEfHbiLijknVKqp7O7tPufqpdm2pTRHwKmA9cm5lLql2PVGkV6waN0hO2f0dp3MNK4CHgmuJOpu72\nnwn8VWZeXLbur4EZwOjM7PYWdEmSpIGski1r5wGLMnNxMaj2VuCqfex/DXse7Nh5x9urKd15JEmS\nNChVMqwdy94PQFzJ3g+EfFZENFG6i+m7Zav/ndJjC/Z1+7okSdKAVis3GMwE7s/MjQDFU7fXZeac\n4rlPPYqI9wDvARgxYsQLTz311ErXKkmSdMjmzJmzITPH72+/Soa1VZSe2N1pMns/vbvc1ZR1gVJ6\nOOOVEXE5pcl7R0fETZn55q5vzMzrgOsAZsyYkQ8//PDhqF2SJKmiImLZ/veqbDfoQ8DJETEtIhop\nBbLnzN1WPGH8ZcAPOtdl5kczc3JmTi3e97PugpokSdJAV7GWtcxsi4j3AXcDdcCNmbkgIt5bbP9q\nsetrgHuKeRAlSZJUZkDNYGA3qCRJ6i8iYk5mztjffs4NKkmSVMMMa5IkSTXMsCZJklTDDGuSJEk1\nzLAmSZJUwwxrkiRJNcywJkmSVMMMa5IkSTXMsCZJklTDDGuSJEk1zLAmSZJUwwxrkiRJNcywJkmS\nVMMMa5IkSTXMsCZJklTDDGuSJEk1zLAmSZJUwwxrkiRJNcywJkmSVMMMa5IkSTXMsCZJklTDDGuS\nJEk1zLAmSZJUwwxrkiRJNcywJkmSVMMMa5IkSTXMsCZJklTDDGuSJEk1zLAmSZJUwwxrkiRJNcyw\nJkmSVMMMa5IkSTXMsCZJklTDDGuSJEk1zLAmSZJUwwxrkiRJNcywJkmSVMMMa5IkSTXMsCZJklTD\nDGuSJEk1zLAmSZJUwwxrkiRJNcywJkmSVMMMa5IkSTXMsCZJklTDDGuSJEk1rKJhLSIujYgnI2JR\nRHykm+0fioi5xc/8iGiPiCMiYkpE/DwiHo+IBRHxgUrWKUmSVKsqFtYiog74MnAZcDpwTUScXr5P\nZl6bmedk5jnAR4FfZuZGoA34m8w8HXgR8Bdd3ytJkjQYVLJl7TxgUWYuzswW4Fbgqn3sfw1wC0Bm\nrs7MR4rXW4GFwLEVrFWSJKkmVTKsHQusKFteSQ+BKyKagEuB73azbSrwfODBHt77noh4OCIeXr9+\n/SGWLEmSVFtq5QaDmcD9RRfosyJiJKUA98HM3NLdGzPzusyckZkzxo8f3welSpKkgSQzaWnrYMuu\nVtZt2cWKjTuqXdJe6it47FXAlLLlycW67lxN0QXaKSIaKAW1mzPztopUKEmSalZbewe72jrY2dLO\nrtbSz87Wdna1dhT/lq1vaWdna8fe67rZ99l1Le3sbiu9b1dbB+0d+eznDm+oY+GnLq3ib763Soa1\nh4CTI2IapZB2NfDGrjtFxBjgZcCby9YFcAOwMDP/tYI1SpKkA9DRkewqCzm9C1Ide97Tzb67i+XO\ndbta2tnV1k5re+6/oG4MrR/CsIY6hjfUMayh9Lpz+ehRDc9ZN6xhSPFv6aepse4wn7VDU7Gwlplt\nEfE+4G6gDrgxMxdExHuL7V8tdn0NcE9mbi97+0uAtwCPRcTcYt3HMvPOStUrSVJ/lZns7gxOz4ai\n7oPQs+ue3bcUpHa1tD8bsp4NTWXLneGspa3joGpsrBvC0LJQVB6kxjY1MrGhjuGNzw1XnUFqaLH8\n7PsbhzC0vvM9e7YNrR/CkCFxmM9wdUXmwaXWWjRjxox8+OGHq12GJElAMRaqvRSidhQ/pddt7Gjd\nE5CeE472GaQ6ng1g5cHqYNQNibIWpb2DVNdgNbzxueu6BqvhjXUMq39ukBpWP4T6uloZJl87ImJO\nZs7Y336V7AaVJKnmdXQkO1vLglRrW1mo6mxVausSttrZ2dp1Xduz+3eu29navtdYqN6IoNtANLyh\njqbGeo4Y0SVYNXYftnoKUsMa9xyvwQDVLxjWJEk1r7W9Yx+h6LlBakdrW5ewVXrfXi1craV1B9oq\nNSSgqbGe4Y2lsU2dQaipsY4jRgylqXN98W9TY30RtDrX1dPUuGdsVNcg1Vg3hNLQbanEsCZJOmSZ\nya7WjucEqfKAtCc0lQWs1i4BrEuQ6lzXdoCtU411Q/aEqc7Q1FDP2KZGJo3tOUh1tl7tHbbqGN5Y\nT1NnV2C9YUp9y7AmSYNEW3vHXoGp+668soDVJUjtam3fe98urVcHMgS6s6vv2VDUsKelamxTA8Mb\n6xneMGRPC1bD3q1SewWphvq9WrOGN9Q5PkoDimFNkvqZ9o7kmZ2tbNrRwuYdLWza3vm69O+mHa1s\n2t6y17rNO1sP+C6+hrp4TkvT8IY6Rg2rZ8LooXsFqWdbn3oIUntarUrHG9Zg65TUW4Y1SaqiXa3t\nZSGrLHBtL0JXeeAq/n1mZ2uPrVj1Q4KxTY2Ma2pgXFMjxx/ZxDlTxjK2qYERQ/ceK7VXwCoLUp1h\ny8HnUm0wrEnSYZCZbN3dxubtrWx8NniVWr0279g7eG3c3vLsup2t7T0es6mxjnFNjYxtauCIEY1M\nHtfEuKaGvcLY2OLfI0aUXo8cWm+LlTTAGNYkqYvW9g427+gasva87mz16ly3uQhhPQ2Cj4Cxw/eE\nq4ljhnHaxNGlwFWErCOaGkshbMSe/YbW19ZT1CVVh2FN0oCVWXp+VucYrvLuxj3jvPYOXpt2tLB1\nV1uPx2ysH7JXq9bJR49k3Ijylq7G57R+jR7eQN0Ae6K6pL5jWJPUL3R0JFt2lboQu7ZqbSp7vXH7\n3gPt9zWoftTQesaO2NOqNe2oEUXIKrVwlQeuzkA2vKHObkZJfcqwJqnP7W4rG1TfZUzX3l2Mew+q\n7+lRW3VDYq/WrClHNHH25DF7haxnQ1jxemxTgwPoJfULhjVJh83utnZ+/sR6Vj+zs/vWr6LrcUdL\nz4PqhzfU7QlXIxqYNHb4XiFrT4tX47NjvkY5qF7SAGZYk3TInt62m5seWM43H1jGhm27gdKg+jFl\ng+qPHjWMUyaM2jt4lQ2o79xvWIOD6iWpnGFN0kH73dqt3Dh7Cbf9dhUtbR1c9LzxvPMl0zjz2DGM\ncVC9JB0WhjVJByQzufepDdwwewn3/m49Q+uH8CcvnMw7XzKVk44eVe3yJGnAMaxJ6pVdre18/7er\nuPH+Jfxu7TbGjxrK377yFN54/vEcMaKx2uVJ0oBlWJO0T+u37uabDyzjpgeWsXF7C6dPHM2/vG46\nV0yf6ENbJakPGNYkdeuJNVu44b4l/GBuM60dHVxy6tG868ITeNEJR3jnpST1IcOapGd1dCS//N16\nrp+9mPsXPc3whjrecO4U3vGSqZwwfmS1y5OkQcmwJomdLe1895GV3Hj/Ehav384xo4fxd5eeyjXn\nTWFsk+PRJKmaDGvSILZuyy7++9fLuOnBZWze0cpZx47hC1efw+VnTfTp/pJUIwxr0iA0f9Uz3Dh7\nCbc/2kxbR/KK0ybw7peewLlTxzkeTZJqjGFNGiQ6OpKfPbGO62cv5oHFG2lqrONN5x/PO14yleOP\nHFHt8iRJPTCsSQPcjpY2vjNnJV+/fylLNmxn0phhfOzyU3nDuccxZnhDtcuTJO2HYU0aoFY/s5Nv\n/GoZt/xmOc/sbGX6lLF88Zrnc+mZxzgeTZL6EcOaNMA8unIzN8xewg8fXU1HJpeeeQzvunAaLzjO\n8WiS1B8Z1qQBoL0j+fHja7lx9hJ+s3QjI4fW87YLpvL2C6Yy5YimapcnSToEhjWpH9u2u43/eXgF\nX79/Kcs37mDyuOF8/NWn8YZzpzBqmOPRJGkgMKxJ/dCqzTv5xq+WcstvlrN1VxsvPH4cH7nsVF55\n+gTqHY8mSQOKYU3qR367fBPXz17CXfPXAHBZMR7t+ceNq3JlkqRKMaxJNa6tvYO7F6zlhtmLeWT5\nZkYNq+fdF07jrRdM5dixw6tdniSpwgxrUo3asquVbz9UGo+2avNOjjuiiU/OPJ0/mTGFkUP96krS\nYOH/4ks1ZsXGHXz9/qV8++EVbNvdxnnTjuAfZ57Oy0+bQN0QH70hSYONYU2qAZnJnGWbuGH2Eu5e\nsIYhEVxx9kTedeEJnDV5TLXLkyRVkWFNqqLW9g5+NH8NN8xewrwVmxkzvIE/e9mJvO3FUzlmzLBq\nlydJqgGGNakKntnZyq2/Wc5//Wopq5/ZxbSjRvCpq87gj184maZGv5aSpD38qyD1oaUbtvP1+5fw\nP3NWsqOlnRefcCSfuupMLj71aIY4Hk2S1A3DmlRhmclvlmzk+tlL+MnCtdQPCWZOn8S7LpzGGZMc\njyZJ2jfDmlQhLW0d/PCxZm6YvYT5q7YwrqmBv7joJN764uM5erTj0SRJvWNYkw6zzTtauPnB5fz3\nr5eydstuThw/gk+/5kxe+/zJDG+sq3Z5kqR+xrAmHSaL12/jxvuX8N05q9jZ2s6FJx3FZ/74bF52\n8njHo0mSDpphTToEmcmvf/80N8xewk+fWEdj3RD+6PmTeOeF0zj1mNHVLk+SNAAY1qSDsLutndvn\nreaG2UtYuHoLR45o5AOXnMybX3Q840cNrXZ5kqQBxLAmHYCN21u4+YFl/PcDy1i/dTenTBjJZ//4\nLK4651iGNTgeTZJ0+BnWpF54au1Wbrx/Cbc9sordbR287JTxvOt103jpyUcR4Xg0SVLlVDSsRcSl\nwBeAOuD6zPxMl+0fAt5UVstpwPjM3Li/90qVlpnMXrSB6+9bwi9/t56h9UN47QuO5Z0vmcbJE0ZV\nuzxJ0iBRsbAWEXXAl4FXACuBhyJiVmY+3rlPZl4LXFvsPxP4qyKo7fe9UqXsam1n1tzS89GeXLuV\no0YO5a9fcQpvOv84jhzpeDRJUt+qZMvaecCizFwMEBG3AlcBPQWua4BbDvK90iHbsG03Nz2wjJse\nWMaGbS2ceswoPv+66cycPpGh9Y5HkyRVRyXD2rHAirLllcD53e0YEU3ApcD7DvS90qF6cs1Wbpi9\nmO/PbaalrYOLTz2ad184jRefeKTj0SRJVVcrNxjMBO7PzI0H+saIeA/wHoDjjjvucNelAaqjI/nl\nU+u5cfYS7ntqA8MahvC6F07mnRdO48TxI6tdniRJz6pkWFsFTClbnlys687V7OkCPaD3ZuZ1wHUA\nM2bMyIMtVoPDrtZ2bntkFTfev4RF67Zx9KihfOhVz+ON5x3HuBGN1S5PkqTnqGRYewg4OSKmUQpa\nVwNv7LpTRIwBXga8+UDfK/XWuq27+Oavl3Hzg8vZuL2FMyaN5t/eMJ1XnzWJxvoh1S5PkqQeVSys\nZWZbRLwPuJvS4zduzMwFEfHeYvtXi11fA9yTmdv3995K1aqB6/HmLdwwewmz5q2irSO55NQJvPul\n0zh/2hGOR5Mk9QuROXB6DmfMmJEPP/xwtctQlXV0JD9/ch03zF7Cr37/NMMb6nj9jMm8/SXTmHbU\niGqXJ0kSABExJzNn7G+/WrnBQDpkO1ra+O4jq/j67CUs3rCdiWOG8ZHLTuWac49jTFNDtcuTJOmg\n7DesRcT7gZsyc1Mf1CMdsDXP7OIbv17Ktx5czjM7Wzl78hi+cPU5XH7WRBrqHI8mSerfetOyNoHS\nDAKPADcCd+dA6jtVv7Wg+Rm+du9i7nh0NR2ZvPL0Y3jXS6cx4/hxjkeTJA0Y+w1rmfnxiPgH4JXA\nO4AvRcS3gRsy8/eVLlDqasuuVq6960luenAZTQ11vOXFx/OOC6Zx3JFN1S5NkqTDrldj1jIzI2IN\nsAZoA8YB34mIH2fmhytZoFTurvlr+MSs+azbupu3XzCVD778FMYMdzyaJGng6s2YtQ8AbwU2ANcD\nH8rM1ogYAjwFGNZUcauf2cknfrCAex5fy2kTR3PdW2YwfcrYapclSVLF9aZl7QjgtZm5rHxlZnZE\nxBWVKUsqae9IbnpgGdfe/SRtHR189LJTeeeF07xxQJI0aPQmrP0IeHbOzogYDZyWmQ9m5sKKVaZB\nb+HqLXz0tseYu2IzLz35KD79R2c5Lk2SNOj0Jqx9BXhB2fK2btZJh82u1na+8NOn+Nq9ixkzvIEv\nXH0OV06f5B2ekqRBqTdhLcof1VF0f/owXVXE7Kc28Pfff4xlT+/g9TMm89HLTnOCdUnSoNab0LU4\nIv6SUmsawJ8DiytXkgajp7ft5tM/XMhtv13FtKNG8K0/PZ8LTjyq2mVJklR1vQlr7wX+A/g4kMBP\ngfdUsigNHpnJdx9Zxad/+DjbdrfxlxefxJ//4UkMa6irdmmSJNWE3jwUdx1wdR/UokFmyYbt/P33\nHuNXv3+aFx4/jn9+7VmcMmFUtcuSJKmm9OY5a8OAdwFnAMM612fmOytYlwawlrYOvnbfYr7w06cY\nWjeET7/mTK459ziGDPEGAkmSuupNN+g3gSeAVwH/B3gT4CM7dFDmLNvEx257jCfXbuXys47hEzPP\nYMLoYft/oyRJg1RvwtpJmfm6iLgqM78REd8C7qt0YRpYyufznDh6GNe/dQYvP31CtcuSJKnm9Sas\ntRb/bo6IMynND3p05UrSQNN1Ps+/eeXzGDnUp79IktQbvfmLeV1EjKN0N+gsYCTwDxWtSgPC6md2\n8o8/WMCPnc9TkqSDts+wVkzWviUzNwH3Aif0SVXq19o7km/+einX3v0k7ZnO5ylJ0iHYZ1grZiv4\nMPDtPqpH/dzC1Vv4yG2PMW/FZv7glPF8+o/OZMoRzucpSdLB6k036E8i4m+B/wds71yZmRt7fosG\nm50tpfk8r7/P+TwlSTqcehPW3lD8+xdl6xK7RFW476n1/P335rN8Y2k+z49dfhpjm5zPU5Kkw6E3\nMxhM64tC1P88vW03//eHC/me83lKklQxvZnB4K3drc/M/z785ag/6JzP8//+8HG2O5+nJEkV1Ztu\n0HPLXg8DLgEeAQxrg1D5fJ4zivk8T3Y+T0mSKqY33aDvL1+OiLHArRWrSDVpr/k8653PU5KkvnIw\nj5HfDjiObRCZs2wTH73tUX63dhuvPmsin5h5Okc7n6ckSX2iN2PWbqd09yfAEOB0fO7aoLBlVyuf\nu+sJbn5wufN5SpJUJb1pWft82es2YFlmrqxQPaoBmcndC9bwjz9YwIZtzucpSVI19eav73JgdWbu\nAoiI4RExNTOXVrQyVUXX+Ty/9lbn85QkqZp6E9b+B7igbLm9WHdu97urP3I+T0mSalNvwlp9ZrZ0\nLmRmS0T4ePoBxPk8JUmqXb0Ja+sj4srMnAUQEVcBGypblvpC53yeX7tvMWOdz1OSpJrUm7D2XuDm\niPhSsbwS6HZWA/Uf9/5uPX///cdYsXGn83lKklTDevNQ3N8DL4qIkcXytopXpYopn8/zhKNGcMuf\nvogXn3hktcuSJEk96M1z1v4J+Fxmbi6WxwF/k5kfr3RxOnwyk+/MWcmn71zofJ6SJPUjvekGvSwz\nP9a5kJmbIuJywLDWTyzZsJ2P3fYYv17sfJ6SJPU3vQlrdRExNDN3Q+k5a8DQypalw6GlrYPr7v09\n//GzRc7nKUlSP9WbsHYz8NOI+DoQwNuBb1SyKB26Ocs28tHbHnM+T0mS+rne3GDw2YiYB7yc0hyh\ndwPHV7owHZyu83ne8LYZXHKa83lKktRf9Xayx7WUgtrrgCXAdytWkQ5KZnLX/DV8YlZpPs93XDCN\nv3nlKYxwPk9Jkvq1Hv+SR8QpwDXFzwbg/wGRmX/YR7Wpl9raO3j/Lb/lR/PXcPrE0Vz/thmcPdn5\nPCVJGgj21ezyBHAfcEVmLgKIiL/qk6p0QO57agM/mr+G9198Eh+45GTqnc9TkqQBY19/1V8LrAZ+\nHhFfi4gQHawTAAAT+klEQVRLKN1goBoza14zo4fV876LTzKoSZI0wPT4lz0zv5+ZVwOnAj8HPggc\nHRFfiYhX9lWB2redLe3cs2ANl505kaH1PuBWkqSBZr/NMJm5PTO/lZkzgcnAb4G/683BI+LSiHgy\nIhZFxEd62OeiiJgbEQsi4pdl6/+qWDc/Im6JCJ870Y2fPbGO7S3tXHnOpGqXIkmSKuCA+swyc1Nm\nXpeZl+xv34ioA74MXAacDlwTEad32Wcs8J/AlZl5BqW7TYmIY4G/BGZk5plAHXD1gdQ6WMyat4rx\no4byohOc31OSpIGokgOczgMWZebizGwBbgWu6rLPG4HbMnM5QGauK9tWDwyPiHqgCWiuYK390pZd\nrfz8yfW8+qyJ1DkrgSRJA1Ilw9qxwIqy5ZXFunKnAOMi4hcRMSci3gqQmauAzwPLKd3k8Exm3lPB\nWvulexaspaWtwy5QSZIGsGrfOlgPvBB4NfAq4B8i4pSIGEepFW4aMAkYERFv7u4AEfGeiHg4Ih5e\nv359X9VdE2bNa2byuOE8f4rPVJMkaaCqZFhbBUwpW55crCu3Eri7uIlhA3AvMJ3S1FZLMnN9ZrYC\ntwEXdPchxRi6GZk5Y/z48Yf9l6hVT2/bzf2LNjBz+iQi7AKVJGmgqmRYewg4OSKmRUQjpRsEZnXZ\n5wfAhRFRHxFNwPnAQkrdny+KiKYoJZFLivUq3PnYato7kiun2wUqSdJAVrGJIzOzLSLeR2ni9zrg\nxsxcEBHvLbZ/NTMXRsRdwKNAB3B9Zs4HiIjvAI8AbZQeF3JdpWrtj2bNa+bko0dy6jGjql2KJEmq\noIrO8p2ZdwJ3dln31S7L1wLXdvPeTwCfqGR9/dWqzTt5aOkm/uYVp9gFKknSAFftGwx0EO6YV3qK\nyUy7QCVJGvAMa/3QrHnNTJ88hqlHjah2KZIkqcIMa/3M79dvY0HzFlvVJEkaJAxr/cysuc1E2AUq\nSdJgYVjrRzKT2+c1c/60I5gw2nntJUkaDAxr/ciC5i0s3rCdK6d3nbVLkiQNVIa1fuT2ec3UDwku\nO/OYapciSZL6iGGtn+joKHWBvvTkoxg3orHa5UiSpD5iWOsn5izfRPMzu7jyHG8skCRpMDGs9ROz\n5jYztH4IrzjdLlBJkgYTw1o/0NbewZ2Preblp01g5NCKzhAmSZJqjGGtH7j/90/z9PYWn60mSdIg\nZFjrB2bNbWbU0Houet74apciSZL6mGGtxu1qbeeeBWt41ZnHMKyhrtrlSJKkPmZYq3G/eHIdW3e3\ncaVdoJIkDUqGtRo3a14zR41s5IITj6x2KZIkqQoMazVs665WfrpwHZefNZH6Oi+VJEmDkQmghv34\n8bXsbuuwC1SSpEHMsFbDbp/XzLFjh/OC48ZVuxRJklQlhrUatWl7C/c9tYErpk9kyJCodjmSJKlK\nDGs16s75q2nrSGaebReoJEmDmWGtRs2a28wJ40dwxqTR1S5FkiRVkWGtBq15Zhe/WbqRK6dPIsIu\nUEmSBjPDWg2649FmMvEuUEmSZFirRbPmNXPmsaM5YfzIapciSZKqzLBWY5Zs2M6jK5+xVU2SJAGG\ntZpz+7xmAK7wLlBJkoRhraZkJrPmNXPe1COYNHZ4tcuRJEk1wLBWQxau3sqidduYeY6tapIkqcSw\nVkNmzWumbkhw+ZnHVLsUSZJUIwxrNSIzuX1eMxeedBRHjhxa7XIkSVKNMKzViEeWb2bV5p3eBSpJ\nkvZiWKsRt89rprF+CK88Y0K1S5EkSTXEsFYD2to7uOPR1Vz8vKMZNayh2uVIkqQaYlirAQ8s3siG\nbbu50rtAJUlSF4a1GjBr3ipGDq3n4lOPrnYpkiSpxhjWqmx3Wzs/mr+GV54+gWENddUuR5Ik1RjD\nWpX98sn1bN3V5oNwJUlStwxrVTZrXjPjmhq48KSjql2KJEmqQYa1Ktq+u42fLFzL5WdNpKHOSyFJ\nkp7LhFBFP1m4ll2tHT4IV5Ik9ciwVkWz5jYzccwwzp16RLVLkSRJNcqwViWbd7Rw71PrueLsiQwZ\nEtUuR5Ik1SjDWpXcNX8Nre3JldOPrXYpkiSphhnWqmTWvGamHTWCM48dXe1SJElSDTOsVcG6Lbv4\n9eKnmXn2RCLsApUkST2raFiLiEsj4smIWBQRH+lhn4siYm5ELIiIX5atHxsR34mIJyJiYUS8uJK1\n9qU7Hl1NJs4FKkmS9qu+UgeOiDrgy8ArgJXAQxExKzMfL9tnLPCfwKWZuTwiyifH/AJwV2b+SUQ0\nAk2VqrWvzZrXzGkTR3PS0aOqXYokSapxlWxZOw9YlJmLM7MFuBW4qss+bwRuy8zlAJm5DiAixgB/\nANxQrG/JzM0VrLXPLH96B3NXbPbZapIkqVcqGdaOBVaULa8s1pU7BRgXEb+IiDkR8dZi/TRgPfD1\niPhtRFwfESMqWGufuf3RZgBmTp9Y5UokSVJ/UO0bDOqBFwKvBl4F/ENEnFKsfwHwlcx8PrAd6GnM\n23si4uGIeHj9+vV9VPbBmzW3mRceP47J4wZMr64kSaqgSoa1VcCUsuXJxbpyK4G7M3N7Zm4A7gWm\nF+tXZuaDxX7foRTeniMzr8vMGZk5Y/z48Yf1FzjcnlyzlSfXbrULVJIk9Volw9pDwMkRMa24QeBq\nYFaXfX4AXBgR9RHRBJwPLMzMNcCKiHhesd8lwOP0c7PmrWJIwOVn2QUqSZJ6p2J3g2ZmW0S8D7gb\nqANuzMwFEfHeYvtXM3NhRNwFPAp0ANdn5vziEO8Hbi6C3mLgHZWqtS9kJrfPW81LTjqK8aOGVrsc\nSZLUT1QsrAFk5p3AnV3WfbXL8rXAtd28dy4wo5L19aW5KzazfOMO3nfxSdUuRZIk9SPVvsFg0Lh9\n3moa64bwqjOOqXYpkiSpHzGs9YH2juSOR5u56HnjGTO8odrlSJKkfsSw1gceXPI067buZqZ3gUqS\npANkWOsDt89rpqmxjpefNqHapUiSpH7GsFZhLW0d3PnYGl5x+gSGN9ZVuxxJktTPGNYq7L6n1vPM\nzlYfhCtJkg6KYa3CZs1rZszwBl56cm3PriBJkmqTYa2Cdra08+PH13L5WcfQWO+pliRJB84EUUE/\nWbiWHS3t3gUqSZIOmmGtgmbNa+boUUM5f9qR1S5FkiT1U4a1CnlmZyu/fHI9V5w9ibohUe1yJElS\nP2VYq5C756+hpb2DK8+xC1SSJB08w1qF3P5oM8cf2cT0yWOqXYokSerHDGsVsH7rbu5ftIGZZ08i\nwi5QSZJ08AxrFXDnY6vpSOwClSRJh8ywVgGz5jXzvAmjOGXCqGqXIkmS+jnD2mG2ctMO5izbZKua\nJEk6LAxrh9nt81YDMPNsw5okSTp0hrXDbNa8Zs6ZMpbjjmyqdimSJGkAMKwdRovWbWXh6i1c6fRS\nkiTpMDGsHUaz5jYzJOCKsydWuxRJkjRAGNYOk8xk1rxmXnTCkRw9eli1y5EkSQOEYe0weWzVMyx9\neoddoJIk6bAyrB0ms+Y201AXXHamXaCSJOnwMawdBh0dyR2PruZlp4xnTFNDtcuRJEkDiGHtMHho\n6UbWbNnFTLtAJUnSYWZYOwxmzWtmeEMdrzh9QrVLkSRJA4xh7RC1tndw52OrueS0o2lqrK92OZIk\naYAxrB2i2Ys2sGlHq3eBSpKkijCsHaLb5zYzelg9L3ve+GqXIkmSBiDD2iHY1drO3QvWcOmZxzC0\nvq7a5UiSpAHIsHYIfvbEOra3tHPl9GOrXYokSRqgDGuHYNbcZo4aOZQXn3hktUuRJEkDlGHtIG3Z\n1crPnlzHFWdPpG5IVLscSZI0QBnWDtI9C9bS0tbhg3AlSVJFGdYO0qx5zUweN5wXHDe22qVIkqQB\nzLB2EJ7etpv7F21g5vRJRNgFKkmSKsewdhDunL+G9o70QbiSJKniDGsH4fa5zZx89EhOPWZUtUuR\nJEkDnGHtADVv3slvlm7kSrtAJUlSHzCsHaA7Hm0G8C5QSZLUJwxrB2jWvGbOnjyGqUeNqHYpkiRp\nEDCsHYDF67cxf9UWbyyQJEl9xrB2AGbNayYCrjjbsCZJkvqGYa2XMpNZ85o5b+oRHDNmWLXLkSRJ\ng0R9JQ8eEZcCXwDqgOsz8zPd7HMR8O9AA7AhM19Wtq0OeBhYlZlXVLLW3vjiNc9nd1tHtcuQJEmD\nSMXCWhG0vgy8AlgJPBQRszLz8bJ9xgL/CVyamcsj4uguh/kAsBAYXak6eysiOGPSmGqXIUmSBplK\ndoOeByzKzMWZ2QLcClzVZZ83Ardl5nKAzFzXuSEiJgOvBq6vYI2SJEk1rZJh7VhgRdnyymJduVOA\ncRHxi4iYExFvLdv278CHAfsdJUnSoFXRMWu9/PwXApcAw4FfR8QDlELcusycU4xp61FEvAd4T7G4\nLSKerFCtRwEbKnRsHTyvS23yutQer0lt8rrUpr66Lsf3ZqdKhrVVwJSy5cnFunIrgaczczuwPSLu\nBaYDLwCujIjLgWHA6Ii4KTPf3PVDMvM64LpK/ALlIuLhzJxR6c/RgfG61CavS+3xmtQmr0ttqrXr\nUslu0IeAkyNiWkQ0AlcDs7rs8wPgwoioj4gm4HxgYWZ+NDMnZ+bU4n0/6y6oSZIkDXQVa1nLzLaI\neB9wN6VHd9yYmQsi4r3F9q9m5sKIuAt4lNLYtOszc36lapIkSepvKjpmLTPvBO7ssu6rXZavBa7d\nxzF+AfyiAuUdqIp3teqgeF1qk9el9nhNapPXpTbV1HWJzKx2DZIkSeqB001JkiTVMMPafkTEpRHx\nZEQsioiPVLuewSwilkbEYxExNyIeLtYdERE/joinin/HVbvOgS4iboyIdRExv2xdj9chIj5afH+e\njIhXVafqga+H6/LJiFhVfGfmFnfYd27zuvSBiJgSET+PiMcjYkFEfKBY73emSvZxTWr2+2I36D4U\nU2b9jrIps4BryqfMUt+JiKXAjMzcULbuc8DGzPxMEabHZebfVavGwSAi/gDYBvx3Zp5ZrOv2OkTE\n6cAtlGY0mQT8BDglM9urVP6A1cN1+SSwLTM/32Vfr0sfiYiJwMTMfCQiRgFzgD8C3o7fmarYxzV5\nPTX6fbFlbd96M2WWqusq4BvF629Q+sKpgjLzXmBjl9U9XYergFszc3dmLgEWUfpe6TDr4br0xOvS\nRzJzdWY+UrzeSmm+62PxO1M1+7gmPan6NTGs7VtvpsxS30ngJ8XUZJ2zVkzIzNXF6zXAhOqUNuj1\ndB38DlXf+yPi0aKbtLOrzetSBRExFXg+8CB+Z2pCl2sCNfp9MaypP7kwM88BLgP+ouj2eVaW+vTt\n168yr0NN+QpwAnAOsBr4l+qWM3hFxEjgu8AHM3NL+Ta/M9XRzTWp2e+LYW3fejNllvpIZq4q/l0H\nfI9SM/TaYvxB5ziEddWrcFDr6Tr4HaqizFybme2Z2QF8jT1dN16XPhQRDZRCwc2ZeVux2u9MFXV3\nTWr5+2JY27feTJmlPhARI4qBoETECOCVwHxK1+NtxW5vozSFmfpeT9dhFnB1RAyNiGnAycBvqlDf\noNQZBgqvofSdAa9Ln4mIAG6gNJXiv5Zt8jtTJT1dk1r+vlR0BoP+rqcps6pc1mA1Afhe6TtGPfCt\nzLwrIh4Cvh0R7wKWUbqbRxUUEbcAFwFHRcRK4BPAZ+jmOhRTzH0beBxoA/7Cu9oqo4frclFEnEOp\ni20p8GfgdeljLwHeAjwWEXOLdR/D70w19XRNrqnV74uP7pAkSaphdoNKkiTVMMOaJElSDTOsSZIk\n1TDDmiRJUg0zrEmSJNUww5okSVINM6xJGrQi4pyIuLxs+cqI+MhhOvYHI6LpcBxL0uDmc9YkDVoR\n8XZgRma+rwLHXloce8MBvKfOB6BK6sqWNUk1LyKmRsTCiPhaRCyIiHsiYngP+54YEXdFxJyIuC8i\nTi3Wvy4i5kfEvIi4t5hC7v8Ab4iIuRHxhoh4e0R8qdj/vyLiKxHxQEQsjoiLIuLGoo7/Kvu8r0TE\nw0Vd/7tY95fAJODnEfHzYt01EfFYUcNny96/LSL+JSLmAS+OiM9ExOMR8WhEfL4yZ1RSf2LLmqSa\nFxFTgUWUWqrmFlO/zMrMm7rZ96fAezPzqYg4H/jnzLw4Ih4DLs3MVRExNjM3d21ZK18uAtkw4Brg\nSuCblKapWUBp3uB3FbUckZkbI6IO+Cnwl5n5aHnLWkRMAh4AXghsAu4B/iMzvx8RCbwhM78dEUcC\nvwJOzczsrPOwn1BJ/Yota5L6iyWZ2TmP3xxgatcdImIkcAHwP8Wcf/8f0Dk58/3Af0XEn1Ka67c3\nbs/S/6N9DFibmY9lZgelwNb5+a+PiEeA3wJnAKd3c5xzgV9k5vrMbANuBv6g2NYOfLd4/QywC7gh\nIl4L7OhlnZIGMCdyl9Rf7C573Q501w06BNicmed03ZCZ7y1a2l4NzImIFx7AZ3Z0+fwOoD4ipgF/\nC5ybmZvKWuMOxK7OcWqZ2RYR5wGXAH8CvA+4+ACPJ2mAsWVN0oCRmVuAJRHxOoAomV68PjEzH8zM\nfwTWA1OArcCoQ/jI0cB24JmImABcVrat/Ni/AV4WEUcV3aXXAL/serCiZXBMZt4J/BUw/RBqkzRA\n2LImaaB5E/CViPg40ADcCswDro2Ik4GgNLZsHrAc+EjRZfrPB/pBmTkvIn4LPAGsoNTV2uk64K6I\naM7MPyweCfLz4vN/mJk/6OaQo4AfRMSwYr+/PtCaJA083mAgSZJUw+wGlSRJqmF2g0rqlyLiy5Qe\npVHuC5n59WrUI0mVYjeoJElSDbMbVJIkqYYZ1iRJkmqYYU2SJKmGGdYkSZJqmGFNkiSphv3/tQTv\n/opa4TkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29d1bf3cf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.figure(figsize=((10,5)))\n",
    "n_estimators = (1, 20, 50, 100, 150, 200, 250)\n",
    "pyplot.errorbar(n_estimators, accuracy)\n",
    "pyplot.ylim([0.64, 0.74])\n",
    "pyplot.title(\"XGBoost: n_estimators vs Accuracy\")\n",
    "pyplot.xlabel('n_estimators')\n",
    "pyplot.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds for example.\n",
    "The general reason is that on most problems, adding more trees beyond a limit does not improve the performance of the model.\n",
    "The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees. Quickly, the model reaches a point of diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2\n",
    "Please complete the code below and try to find the best size of decision trees (max_depth in range from 1 to 9 with step=2)\n",
    "and plot the results. <br> Any ideas why we've got such result? ^-^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "   'classifier__*****': (****)}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=***, scoring=\"accuracy\", cv=kfold)\n",
    "grid_result = ******\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "accuracy = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(accuracy, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "plt.figure(figsize=((10,5)))\n",
    "n_estimators** = (*****)\n",
    "pyplot.errorbar()\n",
    "pyplot.title(\"XGBoost: max_depth vs Accuracy)\n",
    "pyplot.xlabel('****')\n",
    "pyplot.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[To the table of contents](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
